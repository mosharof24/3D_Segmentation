{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import UNet3D\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from monai.utils import first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv3d-1        [1, 16, 64, 64, 64]             448\n",
      "              ReLU-2        [1, 16, 64, 64, 64]               0\n",
      "         Dropout3d-3        [1, 16, 64, 64, 64]               0\n",
      "            Conv3d-4        [1, 16, 64, 64, 64]           6,928\n",
      "              ReLU-5        [1, 16, 64, 64, 64]               0\n",
      "         MaxPool3d-6        [1, 16, 32, 32, 32]               0\n",
      "            Conv3d-7        [1, 32, 32, 32, 32]          13,856\n",
      "              ReLU-8        [1, 32, 32, 32, 32]               0\n",
      "         Dropout3d-9        [1, 32, 32, 32, 32]               0\n",
      "           Conv3d-10        [1, 32, 32, 32, 32]          27,680\n",
      "             ReLU-11        [1, 32, 32, 32, 32]               0\n",
      "        MaxPool3d-12        [1, 32, 16, 16, 16]               0\n",
      "           Conv3d-13        [1, 64, 16, 16, 16]          55,360\n",
      "             ReLU-14        [1, 64, 16, 16, 16]               0\n",
      "        Dropout3d-15        [1, 64, 16, 16, 16]               0\n",
      "           Conv3d-16        [1, 64, 16, 16, 16]         110,656\n",
      "             ReLU-17        [1, 64, 16, 16, 16]               0\n",
      "        MaxPool3d-18           [1, 64, 8, 8, 8]               0\n",
      "           Conv3d-19          [1, 128, 8, 8, 8]         221,312\n",
      "             ReLU-20          [1, 128, 8, 8, 8]               0\n",
      "        Dropout3d-21          [1, 128, 8, 8, 8]               0\n",
      "           Conv3d-22          [1, 128, 8, 8, 8]         442,496\n",
      "             ReLU-23          [1, 128, 8, 8, 8]               0\n",
      "        MaxPool3d-24          [1, 128, 4, 4, 4]               0\n",
      "           Conv3d-25          [1, 256, 4, 4, 4]         884,992\n",
      "             ReLU-26          [1, 256, 4, 4, 4]               0\n",
      "        Dropout3d-27          [1, 256, 4, 4, 4]               0\n",
      "           Conv3d-28          [1, 256, 4, 4, 4]       1,769,728\n",
      "             ReLU-29          [1, 256, 4, 4, 4]               0\n",
      "  ConvTranspose3d-30          [1, 128, 8, 8, 8]         262,272\n",
      "           Conv3d-31          [1, 128, 8, 8, 8]         884,864\n",
      "             ReLU-32          [1, 128, 8, 8, 8]               0\n",
      "        Dropout3d-33          [1, 128, 8, 8, 8]               0\n",
      "           Conv3d-34          [1, 128, 8, 8, 8]         442,496\n",
      "             ReLU-35          [1, 128, 8, 8, 8]               0\n",
      "  ConvTranspose3d-36        [1, 64, 16, 16, 16]          65,600\n",
      "           Conv3d-37        [1, 64, 16, 16, 16]         221,248\n",
      "             ReLU-38        [1, 64, 16, 16, 16]               0\n",
      "        Dropout3d-39        [1, 64, 16, 16, 16]               0\n",
      "           Conv3d-40        [1, 64, 16, 16, 16]         110,656\n",
      "             ReLU-41        [1, 64, 16, 16, 16]               0\n",
      "  ConvTranspose3d-42        [1, 32, 32, 32, 32]          16,416\n",
      "           Conv3d-43        [1, 32, 32, 32, 32]          55,328\n",
      "             ReLU-44        [1, 32, 32, 32, 32]               0\n",
      "        Dropout3d-45        [1, 32, 32, 32, 32]               0\n",
      "           Conv3d-46        [1, 32, 32, 32, 32]          27,680\n",
      "             ReLU-47        [1, 32, 32, 32, 32]               0\n",
      "  ConvTranspose3d-48        [1, 16, 64, 64, 64]           4,112\n",
      "           Conv3d-49        [1, 16, 64, 64, 64]          13,840\n",
      "             ReLU-50        [1, 16, 64, 64, 64]               0\n",
      "        Dropout3d-51        [1, 16, 64, 64, 64]               0\n",
      "           Conv3d-52        [1, 16, 64, 64, 64]           6,928\n",
      "             ReLU-53        [1, 16, 64, 64, 64]               0\n",
      "           Conv3d-54         [1, 5, 64, 64, 64]              85\n",
      "================================================================\n",
      "Total params: 5,644,981\n",
      "Trainable params: 5,644,981\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.00\n",
      "Forward/backward pass size (MB): 483.44\n",
      "Params size (MB): 21.53\n",
      "Estimated Total Size (MB): 505.97\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "device = 'cuda'\n",
    "model = UNet3D()\n",
    "model = model.to(device)\n",
    "summary(model, input_size = (1, 64, 64, 64), batch_size = 1, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./splitted/\n"
     ]
    }
   ],
   "source": [
    "data_dir = './splitted/'\n",
    "print(data_dir)\n",
    "\n",
    "saveFile ='checkpoint'\n",
    "\n",
    "checkpoint_path = saveFile + '.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train image Samples=172\n",
      "Total train image Samples=172\n",
      "Total val image Samples=20\n",
      "Total val mask Samples=20\n"
     ]
    }
   ],
   "source": [
    "# Print the number of training images in the specified directory\n",
    "print('Total train image Samples=' + str(len(glob.glob(data_dir+\"train/images/*.nii\"))))\n",
    "print('Total train image Samples=' + str(len(glob.glob(data_dir+\"train/masks/*.nii\"))))\n",
    "\n",
    "# Print the number of validation images in the specified directory\n",
    "print('Total val image Samples=' + str(len(glob.glob(data_dir+\"test/images/*.nii\"))))\n",
    "print('Total val mask Samples=' + str(len(glob.glob(data_dir+\"test/masks/*.nii\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert integer labels to one-hot encoding\n",
    "def make_one_hot(labels, device, C=2):\n",
    "    '''\n",
    "    Converts integer labels to one-hot encoding for semantic segmentation tasks.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : torch.autograd.Variable of torch.cuda.LongTensor\n",
    "        Shape: N x 1 x H x W, where N is the batch size.\n",
    "        Each value is an integer representing correct classification.\n",
    "    C : integer\n",
    "        Number of classes in labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    target : torch.autograd.Variable of torch.cuda.FloatTensor\n",
    "        Shape: N x C x H x W, where C is the class number. One-hot encoded.\n",
    "    '''\n",
    "    # Ensure labels are of type LongTensor\n",
    "    labels = labels.long()\n",
    "\n",
    "    # Number of classes (including the background class '0')\n",
    "    C = C+1 # add extra 1 for background class (this will be removed later)\n",
    "\n",
    "    # Create a zero-initialized one-hot tensor with the appropriate dimensions\n",
    "    one_hot = torch.FloatTensor(labels.size(0), C, labels.size(2), labels.size(3), labels.size(4)).zero_().to(device)\n",
    "\n",
    "    # Use scatter_ to set the corresponding class index to 1 for each pixel\n",
    "    target = one_hot.scatter_(1, labels.data, 1)\n",
    "\n",
    "    # Convert the result to a torch.autograd.Variable\n",
    "    target = Variable(target)\n",
    "\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read NIfTI image from a given path\n",
    "def readNifti_img(path):\n",
    "    # Load the NIfTI image and normalize pixel values\n",
    "    # print(path)\n",
    "    img_ = nib.load(path).get_fdata()\n",
    "    img_ = img_ / img_.max()\n",
    "    # Convert to torch tensor and add channel dimension\n",
    "    img_ = torch.tensor(img_, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    \n",
    "    # print(img_.shape)\n",
    "    \n",
    "    return img_\n",
    "\n",
    "# Define a function to read NIfTI mask from a given path\n",
    "def readNifti_mask(path):\n",
    "    # Load the NIfTI mask and convert to torch tensor with channel dimension\n",
    "    # print(path)\n",
    "    mask_ = nib.load(path).get_fdata()\n",
    "    # mask_ = mask_ / mask_.max()\n",
    "    mask_ = torch.tensor(mask_, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    return mask_\n",
    "\n",
    "# Define a custom dataset class\n",
    "class NiftiDataset():\n",
    "    def __init__(self, image_paths, mask_paths, transform=None):\n",
    "        # Initialize dataset with image and mask paths, fixed, moving, and transform\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples in the dataset\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the head and tail of the image path\n",
    "        head, tail = os.path.split(self.image_paths[idx])\n",
    "        # print(head)\n",
    "        # print(tail)\n",
    "\n",
    "        # Load fixed image using the readNifti_img function\n",
    "        img = readNifti_img(head + '/' + tail[0:-4] + '.nii')\n",
    "\n",
    "        # Get the head and tail of the mask path\n",
    "        head, tail = os.path.split(self.mask_paths[idx])\n",
    "        # print(head)\n",
    "        # print(tail)\n",
    "\n",
    "        # Load fixed mask using the readNifti_mask function\n",
    "        mask = readNifti_mask(head + '/' + tail[0:-4] + '.nii')\n",
    "\n",
    "        # Create a dictionary containing the fixed and moving images and masks\n",
    "        subject = {'img': img,\n",
    "                   'mask': mask}\n",
    "\n",
    "        # Apply transformations if provided\n",
    "        if self.transform:\n",
    "            subject = self.transform(subject)\n",
    "\n",
    "        # Return the subject dictionary\n",
    "        return subject\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "size of mask before one hot encoding torch.Size([6, 1, 64, 64, 64])\n",
      "size of image torch.Size([6, 1, 64, 64, 64])\n",
      "classes of mask tensor([0., 2., 3., 4., 5.], device='cuda:0')\n",
      "size of mask after one hot encoding torch.Size([6, 6, 64, 64, 64])\n",
      "classes of mask after one hot encodingtensor([0., 1.], device='cuda:0')\n",
      "\n",
      "size of mask before one hot encoding torch.Size([6, 1, 64, 64, 64])\n",
      "size of image torch.Size([6, 1, 64, 64, 64])\n",
      "classes of mask tensor([0., 1., 2., 3., 4., 5.], device='cuda:0')\n",
      "size of mask after one hot encoding torch.Size([6, 6, 64, 64, 64])\n",
      "classes of mask after one hot encodingtensor([0., 1.], device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a DataLoader for the training dataset\n",
    "train_loader = DataLoader(\n",
    "    NiftiDataset(\n",
    "        sorted(glob.glob(data_dir+\"train/images/*.nii\")),\n",
    "        sorted(glob.glob(data_dir+\"train/masks/*.nii\")),\n",
    "        transform=None\n",
    "    ),\n",
    "    batch_size=6,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# Create a DataLoader for the testing/validation dataset\n",
    "val_loader = DataLoader(\n",
    "    NiftiDataset(\n",
    "        sorted(glob.glob(data_dir+\"test/images/*.nii\")),\n",
    "        sorted(glob.glob(data_dir+\"test/masks/*.nii\")),\n",
    "        transform=None\n",
    "    ),\n",
    "    batch_size=6,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print()\n",
    "\n",
    "# Retrieve and print a sample from the training dataset\n",
    "train_sample = first(train_loader)\n",
    "print(f\"size of mask before one hot encoding {train_sample['mask'].shape}\")\n",
    "print(f\"size of image {train_sample['img'].shape}\")\n",
    "print(f\"classes of mask {torch.unique(train_sample['mask'])}\")\n",
    "train_sample['mask'] = make_one_hot(train_sample['mask'], device, C=5)\n",
    "print(f\"size of mask after one hot encoding {train_sample['mask'].shape}\")\n",
    "print(f\"classes of mask after one hot encoding{torch.unique(train_sample['mask'])}\")\n",
    "print()\n",
    "\n",
    "# Retrieve and print a sample from the testing dataset\n",
    "test_sample = first(val_loader)\n",
    "print(f\"size of mask before one hot encoding {test_sample['mask'].shape}\")\n",
    "print(f\"size of image {test_sample['img'].shape}\")\n",
    "print(f\"classes of mask {torch.unique(test_sample['mask'])}\")\n",
    "test_sample['mask'] = make_one_hot(test_sample['mask'], device, C=5)\n",
    "print(f\"size of mask after one hot encoding {test_sample['mask'].shape}\")\n",
    "print(f\"classes of mask after one hot encoding{torch.unique(test_sample['mask'])}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1.], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unique(test_sample['mask'][:, 1, :, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss() # binary cross-entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 10\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        Xbatch = X[i:i+batch_size]\n",
    "        y_pred = model(Xbatch)\n",
    "        ybatch = y[i:i+batch_size]\n",
    "        loss = loss_fn(y_pred, ybatch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Finished epoch {epoch}, latest loss {loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miccai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
